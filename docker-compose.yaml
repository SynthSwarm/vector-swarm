services:
  llm-engine:
    image: vllm/vllm-openai:v0.6.6  # CUDA 12.1 compatible (works with 12.8 driver)
    container_name: vllm_inference_engine
    ports:
      - "8000:8000"  # vLLM OpenAI-compatible API
    volumes:
      - vllm_models:/root/.cache/huggingface  # HuggingFace model cache
    environment:
      # Optional: Set HuggingFace token for gated models (not needed for Qwen)
      # - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/root/.cache/huggingface

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Shared memory for PyTorch (critical for vLLM)
    shm_size: '2gb'

    restart: unless-stopped

    # Custom entrypoint that pre-downloads model then starts vLLM
    entrypoint: ["/bin/bash", "-c", "/init-vllm.sh"]

    configs:
      - source: init_script
        target: /init-vllm.sh
        mode: 0755

volumes:
  vllm_models:
    driver: local

configs:
  init_script:
    content: |
      #!/bin/bash
      set -e

      echo "==================================="
      echo "vLLM Initialization Script"
      echo "==================================="

      # Model configuration
      MODEL_NAME="Qwen/Qwen3-0.6B"
      CACHE_DIR="/root/.cache/huggingface"

      echo "Installing HuggingFace CLI..."
      pip install -q huggingface_hub

      echo ""
      echo "Checking for model: $$MODEL_NAME"

      # Check if model is already cached
      MODEL_DIR="$$CACHE_DIR/hub/models--Qwen--Qwen3-0.6B"
      if [ -d "$$MODEL_DIR" ]; then
        echo "✓ Model already cached at $$MODEL_DIR"
      else
        echo "Downloading model from HuggingFace..."
        echo "This may take a few minutes (model size: ~600MB)..."

        # Use huggingface-cli for cleaner download
        huggingface-cli download "$$MODEL_NAME" --cache-dir "$$CACHE_DIR" --resume-download

        if [ $$? -eq 0 ]; then
          echo "✓ Model download complete!"
        else
          echo "✗ Error downloading model"
          exit 1
        fi
      fi

      echo ""
      echo "==================================="
      echo "Starting vLLM Server..."
      echo "==================================="
      echo "Model: $$MODEL_NAME"
      echo "API Port: 8000"
      echo "GPU Memory: ~75% (configurable)"
      echo "==================================="
      echo ""

      # Start vLLM with optimizations for small GPU
      exec python3 -m vllm.entrypoints.openai.api_server \
        --model "$$MODEL_NAME" \
        --host 0.0.0.0 \
        --port 8000 \
        --gpu-memory-utilization 0.75 \
        --max-model-len 2048 \
        --dtype auto \
        --trust-remote-code
